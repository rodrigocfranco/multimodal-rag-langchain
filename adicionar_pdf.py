#!/usr/bin/env python3
"""
Adicionar PDF ao Knowledge Base
Sistema √∫nico e simples com metadados otimizados + Metadata Enrichment
"""

import os
import sys
from dotenv import load_dotenv
import time
from document_manager import generate_pdf_id, check_duplicate
from PIL import Image
import io
from base64 import b64decode, b64encode

load_dotenv()

# ===========================================================================
# METADATA ENRICHMENT SYSTEM
# ===========================================================================
print("üöÄ Carregando Metadata Enrichment System...")
from metadata_extractors import MetadataEnricher

# Inicializar extractors globais (uma vez s√≥ para melhor performance)
enricher = MetadataEnricher()
print()

if len(sys.argv) < 2:
    print("Uso: python adicionar_pdf.py arquivo.pdf")
    print("   ou: python adicionar_pdf.py content/arquivo.pdf")
    exit(1)

# Aceitar tanto "arquivo.pdf" quanto "content/arquivo.pdf"
input_path = sys.argv[1]

if os.path.exists(input_path):
    # Caminho completo fornecido
    file_path = input_path
    pdf_filename = os.path.basename(input_path)
elif os.path.exists(f"./content/{input_path}"):
    # S√≥ nome do arquivo fornecido
    file_path = f"./content/{input_path}"
    pdf_filename = input_path
else:
    print(f"‚ùå PDF n√£o encontrado: {input_path}")
    print(f"   Tentou tamb√©m: ./content/{input_path}")
    exit(1)

print(f"üìÑ Processando: {pdf_filename}")
print("‚è≥ Aguarde 5-10 minutos...\n")

# Vectorstore unificado - Railway Volume
persist_directory = os.getenv("PERSIST_DIR", "./knowledge")

# ===========================================================================
# GERAR PDF_ID E VERIFICAR DUPLICATA
# ===========================================================================
print("üîç Gerando ID do documento...")
pdf_id = generate_pdf_id(file_path)
file_size = os.path.getsize(file_path)
uploaded_at = time.strftime("%Y-%m-%d %H:%M:%S")

print(f"   PDF_ID: {pdf_id[:16]}...")
print(f"   Tamanho: {file_size / 1024 / 1024:.2f} MB")

# Verificar se PDF j√° foi processado
existing_doc = check_duplicate(file_path, persist_directory)
if existing_doc:
    print(f"\n‚ö†Ô∏è  Este PDF j√° foi processado!")
    print(f"   Adicionado em: {existing_doc.get('uploaded_at', 'desconhecido')}")
    print(f"   Chunks: {existing_doc.get('stats', {}).get('total_chunks', 0)}")

    if os.getenv("AUTO_REPROCESS") != "true":
        choice = input("\nReprocessar? (s/N): ")
        if choice.lower() != 's':
            print("‚ùå Processamento cancelado.")
            exit(0)
        print("\nüîÑ Reprocessando documento...\n")
    else:
        print("\nüîÑ AUTO_REPROCESS=true, reprocessando automaticamente...\n")
else:
    print("‚úÖ Documento novo, prosseguindo...\n")

# ===========================================================================
# EXTRAIR E PROCESSAR PDF
# ===========================================================================
from unstructured.partition.pdf import partition_pdf

# Permite alternar a estrat√©gia via vari√°vel de ambiente e faz fallback autom√°tico
strategy_env = os.getenv("UNSTRUCTURED_STRATEGY", "hi_res").strip().lower()

def run_partition(strategy: str):
    return partition_pdf(
        filename=file_path,
        infer_table_structure=True,
        strategy=strategy,
        extract_image_block_types=["Image", "Table"],
        extract_image_block_to_payload=True,
        languages=["por"],  # ‚úÖ For√ßa OCR em portugu√™s

        # ‚úÖ CHUNKING OTIMIZADO PARA DOCUMENTOS M√âDICOS
        # NOTA IMPORTANTE: Tabelas s√£o SEMPRE preservadas inteiras (isoladas)
        # tanto em by_title quanto em basic - ver documenta√ß√£o Unstructured
        chunking_strategy="by_title",

        # Hard maximum: ~2500 tokens - chunks grandes preservam contexto completo
        max_characters=10000,

        # Agrupa elementos pequenos (<4000 chars) no mesmo chunk
        # Combina m√∫ltiplos par√°grafos relacionados da mesma se√ß√£o
        combine_text_under_n_chars=4000,

        # Soft maximum: for√ßa quebra em 6000 chars (~1500 tokens)
        # Balanceia contexto amplo com efici√™ncia de retrieval
        new_after_n_chars=6000,
    )

try:
    # Tenta com a estrat√©gia definida (padr√£o hi_res)
    chunks = run_partition(strategy_env)
    strategy_used = strategy_env
except Exception as e:
    # Se falhar por falta de libGL/cv2, faz fallback para 'fast'
    if "libGL.so.1" in str(e) or "cv2" in str(e) or "detectron2onnx" in str(e):
        print("‚ö†Ô∏è  Falha em hi_res (prov√°vel falta de libGL). Usando strategy='fast'.")
        chunks = run_partition("fast")
        strategy_used = "fast"
    else:
        raise

print(f"1Ô∏è‚É£  Extra√≠do: {len(chunks)} elementos (estrat√©gia: {strategy_used})")

# DEBUG: Mostrar tipos de elementos
element_types = {}
for chunk in chunks:
    chunk_type = str(type(chunk).__name__)
    element_types[chunk_type] = element_types.get(chunk_type, 0) + 1

print(f"\n   Tipos de elementos encontrados:")
for elem_type, count in sorted(element_types.items()):
    print(f"     {elem_type}: {count}")

# Separar elementos
# Com chunking by_title, Unstructured retorna:
# - CompositeElement: textos agrupados por se√ß√£o
# - Table: tabelas isoladas (sempre preservadas inteiras)
#
# NOTA: Com par√¢metros agressivos de chunking, tabelas podem vir:
# 1. Como elementos Table de primeira classe (ideal)
# 2. Dentro de CompositeElement.metadata.orig_elements (com chunking agressivo)
tables = []
texts = []

for chunk in chunks:
    chunk_type = str(type(chunk))

    # Tabelas diretas (primeira classe)
    if "Table" in chunk_type:
        tables.append(chunk)

    # CompositeElements (textos agrupados)
    if "CompositeElement" in chunk_type:
        texts.append(chunk)

        # CRITICAL: Verificar se h√° tabelas escondidas em orig_elements
        # Com chunking agressivo, tabelas podem ser agrupadas aqui
        if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):
            if chunk.metadata.orig_elements:
                for orig_el in chunk.metadata.orig_elements:
                    if "Table" in str(type(orig_el).__name__):
                        # Tabela encontrada dentro de CompositeElement
                        # Adicionar √† lista de tabelas
                        if orig_el not in tables:
                            tables.append(orig_el)
                            print(f"   ‚ö†Ô∏è  Tabela encontrada em orig_elements (chunk agressivo)")

# ===========================================================================
# FUN√á√ïES DE EXTRA√á√ÉO DE METADATA M√âDICO
# ===========================================================================
def extract_section_heading(text_element):
    """
    Extrai section heading de elementos de texto
    Detecta se√ß√µes m√©dicas comuns em artigos cient√≠ficos

    Args:
        text_element: Elemento de texto do Unstructured

    Returns:
        str: Nome da se√ß√£o (Title case) ou None se n√£o detectado
    """
    # Verificar se elemento tem metadata
    if not hasattr(text_element, 'metadata'):
        return None

    # Unstructured detecta categorias automaticamente
    if hasattr(text_element.metadata, 'category'):
        cat = text_element.metadata.category

        # Se for Title, tentar identificar qual se√ß√£o
        if cat == 'Title':
            text = text_element.text if hasattr(text_element, 'text') else str(text_element)
            text_lower = text.lower().strip()

            # Se√ß√µes m√©dicas/cient√≠ficas comuns (em portugu√™s e ingl√™s)
            medical_sections = {
                # Portugu√™s
                'resumo': 'Resumo',
                'abstract': 'Abstract',
                'introdu√ß√£o': 'Introdu√ß√£o',
                'introduction': 'Introduction',
                'contexto': 'Contexto',
                'background': 'Background',
                'objetivos': 'Objetivos',
                'objectives': 'Objectives',
                'm√©todos': 'M√©todos',
                'metodologia': 'Metodologia',
                'methods': 'Methods',
                'methodology': 'Methodology',
                'materiais e m√©todos': 'Materiais e M√©todos',
                'materials and methods': 'Materials and Methods',
                'resultados': 'Resultados',
                'results': 'Results',
                'discuss√£o': 'Discuss√£o',
                'discussion': 'Discussion',
                'conclus√£o': 'Conclus√£o',
                'conclus√µes': 'Conclus√£o',  # Mapeia para singular
                'conclusion': 'Conclusion',
                'conclusions': 'Conclusion',  # Mapeia para singular
                'refer√™ncias': 'Refer√™ncias',
                'references': 'References',
                'bibliografia': 'Bibliografia',
                'agradecimentos': 'Agradecimentos',
                'acknowledgments': 'Acknowledgments',
                'acknowledgements': 'Acknowledgements',
                # Se√ß√µes m√©dicas espec√≠ficas
                'relato de caso': 'Relato de Caso',
                'case report': 'Case Report',
                'apresenta√ß√£o do caso': 'Apresenta√ß√£o do Caso',
                'case presentation': 'Case Presentation',
                'achados cl√≠nicos': 'Achados Cl√≠nicos',
                'clinical findings': 'Clinical Findings',
                'diagn√≥stico': 'Diagn√≥stico',
                'diagnosis': 'Diagnosis',
                'diagn√≥stico diferencial': 'Diagn√≥stico Diferencial',
                'differential diagnosis': 'Differential Diagnosis',
                'tratamento': 'Tratamento',
                'treatment': 'Treatment',
                'terap√™utica': 'Terap√™utica',
                'therapeutics': 'Therapeutics',
                'manejo': 'Manejo',
                'management': 'Management',
                'evolu√ß√£o': 'Evolu√ß√£o',
                'outcome': 'Outcome',
                'desfecho': 'Desfecho',
                'follow-up': 'Follow-up',
                'acompanhamento': 'Acompanhamento',
                'complica√ß√µes': 'Complica√ß√µes',
                'complications': 'Complications',
                'efeitos adversos': 'Efeitos Adversos',
                'adverse effects': 'Adverse Effects',
            }

            # Procurar match exato ou por substring
            for key, value in medical_sections.items():
                if key in text_lower:
                    return value

    return None


def infer_document_type(filename):
    """
    Infere tipo de documento m√©dico pelo nome do arquivo

    Args:
        filename: Nome do arquivo PDF

    Returns:
        str: Tipo do documento (lowercase com underscore)
    """
    filename_lower = filename.lower()

    # Artigos de revis√£o
    if any(word in filename_lower for word in ['artigo de revis√£o', 'review article', 'review -', '- review']):
        return 'review_article'

    # Guidelines / Diretrizes
    if any(word in filename_lower for word in ['guideline', 'diretriz', 'consenso', 'consensus', 'recomenda√ß√µes']):
        return 'clinical_guideline'

    # Relatos de caso
    if any(word in filename_lower for word in ['case report', 'relato de caso', 'case series']):
        return 'case_report'

    # Ensaios cl√≠nicos / RCTs
    if any(word in filename_lower for word in ['rct', 'trial', 'ensaio cl√≠nico', 'clinical trial', 'randomized']):
        return 'clinical_trial'

    # Meta-an√°lises
    if any(word in filename_lower for word in ['meta-analysis', 'metan√°lise', 'meta-an√°lise', 'systematic review', 'revis√£o sistem√°tica']):
        return 'meta_analysis'

    # Estudos de coorte
    if any(word in filename_lower for word in ['cohort', 'coorte', 'prospective study', 'estudo prospectivo']):
        return 'cohort_study'

    # Estudos observacionais
    if any(word in filename_lower for word in ['observational', 'observacional', 'cross-sectional', 'transversal']):
        return 'observational_study'

    # Artigos originais / pesquisa
    if any(word in filename_lower for word in ['original article', 'artigo original', 'research article', 'research paper']):
        return 'original_research'

    # Editoriais / Coment√°rios
    if any(word in filename_lower for word in ['editorial', 'commentary', 'coment√°rio', 'perspective']):
        return 'editorial'

    # Default: artigo m√©dico gen√©rico
    return 'medical_article'


# ===========================================================================
# CONVERS√ÉO DE IMAGENS PARA FORMATO SUPORTADO
# ===========================================================================

def convert_image_to_jpeg_base64(image_base64_str):
    """
    Converte qualquer formato de imagem para JPEG (suportado por GPT-4 Vision).

    Formatos n√£o suportados: TIFF, BMP, ICO, etc.
    Formatos suportados: PNG, JPEG, GIF, WEBP

    Esta fun√ß√£o garante que TODAS as imagens sejam JPEG v√°lidas.
    """
    try:
        # Decodificar base64 para bytes
        image_bytes = b64decode(image_base64_str)

        # Abrir imagem com PIL
        img = Image.open(io.BytesIO(image_bytes))

        # Converter para RGB (remove alpha channel se houver)
        # Isso √© necess√°rio porque JPEG n√£o suporta transpar√™ncia
        if img.mode in ('RGBA', 'LA', 'P'):
            # Criar background branco
            background = Image.new('RGB', img.size, (255, 255, 255))

            # Converter P (palette) para RGBA primeiro
            if img.mode == 'P':
                img = img.convert('RGBA')

            # Colar imagem sobre background branco (preserva transpar√™ncia)
            if img.mode in ('RGBA', 'LA'):
                background.paste(img, mask=img.split()[-1])  # Usa alpha channel como m√°scara
            else:
                background.paste(img)

            img = background
        elif img.mode != 'RGB':
            # Outros modos (L, CMYK, etc.) ‚Üí RGB
            img = img.convert('RGB')

        # Salvar como JPEG em buffer
        output = io.BytesIO()
        img.save(output, format='JPEG', quality=85, optimize=True)
        jpeg_bytes = output.getvalue()

        # Re-encodar para base64
        jpeg_base64 = b64encode(jpeg_bytes).decode('utf-8')

        return jpeg_base64, True

    except Exception as e:
        # Se convers√£o falhar, retornar None
        print(f"      ‚ö†Ô∏è  Erro ao converter imagem: {str(e)[:100]}")
        return None, False


# Extrair imagens base64 dos chunks
def get_images_base64(chunks):
    """
    Extrai imagens de:
    1. Elementos Image de primeira classe (diretos)
    2. Imagens dentro de CompositeElement.metadata.orig_elements

    Filtra imagens pequenas (<5KB) que geralmente s√£o √≠cones/decora√ß√£o

    ‚úÖ CONVERTE TODAS AS IMAGENS PARA JPEG (formato suportado por GPT-4 Vision)
    """
    images_b64 = []
    seen_hashes = set()  # Deduplica√ß√£o
    filtered_count = 0
    total_found = 0

    # Filtro: imagens muito pequenas geralmente s√£o √≠cones/decora√ß√£o
    # Aumentado para 30KB ap√≥s an√°lise: imagens reais (figuras m√©dicas) geralmente >30KB
    # Imagens 5-30KB s√£o tipicamente: √≠cones, logos, decora√ß√µes, gr√°ficos simples
    MIN_IMAGE_SIZE_KB = float(os.getenv("MIN_IMAGE_SIZE_KB", "30"))

    for chunk in chunks:
        chunk_type = str(type(chunk))

        # CASO 1: Elementos Image de primeira classe
        if "Image" in chunk_type:
            total_found += 1
            if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'image_base64'):
                img = chunk.metadata.image_base64
                if img and len(img) > 100:
                    # ‚úÖ CONVERT TO JPEG BEFORE SIZE CHECK
                    jpeg_img, success = convert_image_to_jpeg_base64(img)
                    if not success:
                        filtered_count += 1
                        continue

                    size_kb = len(jpeg_img) / 1024

                    # Filtrar imagens muito pequenas
                    if size_kb >= MIN_IMAGE_SIZE_KB:
                        # Deduplicar por hash
                        img_hash = hash(jpeg_img[:1000])
                        if img_hash not in seen_hashes:
                            seen_hashes.add(img_hash)
                            images_b64.append(jpeg_img)  # ‚úÖ Use converted JPEG
                    else:
                        filtered_count += 1

        # CASO 2: Imagens dentro de CompositeElements
        elif "CompositeElement" in chunk_type:
            if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):
                chunk_els = chunk.metadata.orig_elements
                if chunk_els:
                    for el in chunk_els:
                        if "Image" in str(type(el)):
                            total_found += 1
                            if hasattr(el, 'metadata') and hasattr(el.metadata, 'image_base64'):
                                img = el.metadata.image_base64
                                if img and len(img) > 100:
                                    # ‚úÖ CONVERT TO JPEG BEFORE SIZE CHECK
                                    jpeg_img, success = convert_image_to_jpeg_base64(img)
                                    if not success:
                                        filtered_count += 1
                                        continue

                                    size_kb = len(jpeg_img) / 1024

                                    # Filtrar imagens muito pequenas
                                    if size_kb >= MIN_IMAGE_SIZE_KB:
                                        # Deduplicar por hash
                                        img_hash = hash(jpeg_img[:1000])
                                        if img_hash not in seen_hashes:
                                            seen_hashes.add(img_hash)
                                            images_b64.append(jpeg_img)  # ‚úÖ Use converted JPEG
                                    else:
                                        filtered_count += 1

    return images_b64, filtered_count, total_found

images, filtered_count, total_images_found = get_images_base64(chunks)
duplicate_count = 0  # J√° deduplicado na fun√ß√£o

# Modo debug: mostrar detalhes das imagens
if os.getenv("DEBUG_IMAGES"):
    print("\n   [DEBUG] Detalhes das imagens extra√≠das:")
    for i, img in enumerate(images):
        size_kb = len(img) / 1024
        print(f"     Imagem {i+1}: {size_kb:.1f} KB")
    print(f"   [DEBUG] Total encontrado: {total_images_found}")
    print(f"   [DEBUG] Filtradas (muito pequenas): {filtered_count}")
    print(f"   [DEBUG] Duplicatas removidas: {duplicate_count}")

print(f"   ‚úì {len(texts)} textos, {len(tables)} tabelas, {len(images)} imagens")
if filtered_count > 0:
    print(f"      (detectadas: {total_images_found}, filtradas: {filtered_count} imagens pequenas <30KB)")
print()

# ===========================================================================
# EXTRA√á√ÉO ROBUSTA DE TABELAS COM VISION API
# ===========================================================================
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

def validate_table_completeness(table_text, critical_keywords=None):
    """
    Valida se tabela est√° completa baseado em keywords esperadas

    Args:
        table_text: Texto extra√≠do da tabela
        critical_keywords: Lista de keywords que DEVEM estar presentes

    Returns:
        dict com completeness score e keywords faltando
    """
    if not critical_keywords:
        # Keywords m√©dicos comuns em tabelas de diretrizes
        critical_keywords = [
            "muito alto", "alto", "moderado", "baixo",  # Classifica√ß√µes
            "hipercolesterolemia", "albumin√∫ria", "TFG",  # Termos m√©dicos
            "fatores de risco", "crit√©rios"  # Estrutura
        ]

    missing = []
    for keyword in critical_keywords:
        if keyword.lower() not in table_text.lower():
            missing.append(keyword)

    completeness = 1 - (len(missing) / len(critical_keywords))

    return {
        "complete": len(missing) == 0,
        "completeness": completeness,
        "missing_keywords": missing,
        "total_keywords": len(critical_keywords)
    }


def extract_table_with_vision(table_element, pdf_filename):
    """
    Extrai tabela usando GPT-4o Vision - M√âTODO ROBUSTO

    Args:
        table_element: Elemento Table do Unstructured
        pdf_filename: Nome do arquivo PDF (para contexto)

    Returns:
        tuple: (vision_text, success, metadata)
    """
    # Verificar se tabela tem imagem
    if not hasattr(table_element, 'metadata') or not hasattr(table_element.metadata, 'image_base64'):
        return None, False, {"error": "No image available"}

    image_b64 = table_element.metadata.image_base64
    if not image_b64 or len(image_b64) < 100:
        return None, False, {"error": "Image too small"}

    # ‚úÖ CONVERT TABLE IMAGE TO JPEG (fix unsupported formats)
    jpeg_image_b64, success = convert_image_to_jpeg_base64(image_b64)
    if not success:
        return None, False, {"error": "Failed to convert image to JPEG"}

    image_b64 = jpeg_image_b64  # Use converted image

    page_num = table_element.metadata.page_number if hasattr(table_element.metadata, 'page_number') else '?'

    try:
        llm = ChatOpenAI(model="gpt-4o", max_tokens=2000, temperature=0)

        prompt = f"""Voc√™ √© um especialista em extra√ß√£o de tabelas m√©dicas.

TAREFA CR√çTICA: Extraia COMPLETAMENTE esta tabela, preservando TODAS as colunas e linhas.

INSTRU√á√ïES:
1. Identifique TODAS as colunas (mesmo se parecerem vazias ou com pouco texto)
2. Preserve a estrutura EXATA da tabela
3. Use formato Markdown table
4. Se houver headers, identifique-os claramente
5. IMPORTANTE: N√ÉO omita nenhuma coluna - tabelas m√©dicas frequentemente t√™m colunas sutis

CONTEXTO:
- Documento: {pdf_filename}
- P√°gina: {page_num}
- Tipo: Diretriz m√©dica/cient√≠fica

FORMATO DE SA√çDA:
Forne√ßa a tabela em Markdown, come√ßando com | Coluna1 | Coluna2 | ... |

TABELA:"""

        message = HumanMessage(
            content=[
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}
                }
            ]
        )

        response = llm.invoke([message])
        vision_text = response.content

        return vision_text, True, {
            "page": page_num,
            "length": len(vision_text),
            "method": "gpt-4o-vision"
        }

    except Exception as e:
        return None, False, {"error": str(e)[:100]}


def extract_table_robust(table_element, pdf_filename, table_index):
    """
    EXTRA√á√ÉO ROBUSTA: OCR + Vision + Valida√ß√£o + Decis√£o Inteligente

    Esta √© a fun√ß√£o DEFINITIVA para extra√ß√£o de tabelas.

    Args:
        table_element: Elemento Table do Unstructured
        pdf_filename: Nome do PDF
        table_index: √çndice da tabela (para logging)

    Returns:
        tuple: (final_text, method_used, quality_report)
    """
    # 1. Extrair com OCR (Unstructured)
    ocr_text = table_element.text if hasattr(table_element, 'text') else str(table_element)
    ocr_length = len(ocr_text.split())

    # 2. Extrair com Vision (GPT-4o)
    vision_text, vision_success, vision_meta = extract_table_with_vision(table_element, pdf_filename)
    vision_length = len(vision_text.split()) if vision_success else 0

    # 3. Validar completude de AMBOS
    ocr_validation = validate_table_completeness(ocr_text)
    vision_validation = validate_table_completeness(vision_text) if vision_success else {"complete": False, "completeness": 0}

    # 4. DECIS√ÉO INTELIGENTE: Qual m√©todo usar?

    # Caso 1: Vision falhou -> usar OCR (sem escolha)
    if not vision_success:
        method = "ocr_only"
        final_text = ocr_text
        confidence = "low" if ocr_validation["completeness"] < 0.7 else "medium"

    # Caso 2: OCR perdeu >30% do conte√∫do -> usar Vision
    elif vision_length > 0 and ocr_length < 0.7 * vision_length:
        method = "vision_primary"
        final_text = f"{vision_text}\n\n[OCR BACKUP]\n{ocr_text}"
        confidence = "high"

    # Caso 3: Vision tem mais keywords cr√≠ticos -> usar Vision
    elif vision_validation["completeness"] > ocr_validation["completeness"]:
        method = "vision_primary"
        final_text = f"{vision_text}\n\n[OCR BACKUP]\n{ocr_text}"
        confidence = "high"

    # Caso 4: OCR est√° OK -> usar OCR (mais barato)
    else:
        method = "ocr_primary"
        final_text = f"{ocr_text}\n\n[VISION BACKUP]\n{vision_text}" if vision_success else ocr_text
        confidence = "high" if ocr_validation["completeness"] > 0.8 else "medium"

    # 5. Relat√≥rio de qualidade
    quality_report = {
        "table_index": table_index,
        "method_used": method,
        "confidence": confidence,
        "ocr": {
            "length_words": ocr_length,
            "completeness": ocr_validation["completeness"],
            "missing": ocr_validation["missing_keywords"]
        },
        "vision": {
            "success": vision_success,
            "length_words": vision_length,
            "completeness": vision_validation["completeness"],
            "missing": vision_validation.get("missing_keywords", [])
        } if vision_success else {"success": False},
        "final_length": len(final_text.split())
    }

    return final_text, method, quality_report


# Processar TODAS as tabelas com extra√ß√£o robusta
if tables:
    print(f"\nüî¨ Processamento robusto de tabelas (OCR + Vision)...")

    tables_quality_reports = []
    vision_used_count = 0
    ocr_only_count = 0

    for i, table in enumerate(tables):
        # Extrair com m√©todo robusto
        robust_text, method, quality = extract_table_robust(table, pdf_filename, i)

        # Atualizar texto da tabela com vers√£o robusta
        if hasattr(table, 'text'):
            table.text = robust_text
        else:
            # Criar wrapper se necess√°rio
            class TableWithText:
                def __init__(self, text, metadata):
                    self.text = text
                    self.metadata = metadata

            original_metadata = table.metadata if hasattr(table, 'metadata') else None
            tables[i] = TableWithText(robust_text, original_metadata)

        # Tracking
        tables_quality_reports.append(quality)
        if "vision" in method:
            vision_used_count += 1
        else:
            ocr_only_count += 1

        # Log progress
        status_icon = "‚úÖ" if quality["confidence"] == "high" else "‚ö†Ô∏è"
        print(f"   [{i+1}/{len(tables)}] {status_icon} {method} (confidence: {quality['confidence']})")

        # Mostrar warnings
        if quality["ocr"]["missing"]:
            print(f"      OCR missing: {', '.join(quality['ocr']['missing'][:3])}")
        if quality["vision"]["success"] and quality["vision"]["missing"]:
            print(f"      Vision missing: {', '.join(quality['vision']['missing'][:3])}")

        time.sleep(0.5)  # Rate limiting

    print(f"\n   üìä Resumo:")
    print(f"      Vision usado: {vision_used_count}/{len(tables)} tabelas")
    print(f"      OCR apenas: {ocr_only_count}/{len(tables)} tabelas")
    print(f"      Confian√ßa alta: {sum(1 for r in tables_quality_reports if r['confidence'] == 'high')}/{len(tables)}")
    print()

# ===========================================================================
# GERAR RESUMOS COM IA
# ===========================================================================
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

print("2Ô∏è‚É£  Gerando resumos...")

# Upgrade: Llama ‚Üí GPT-4o-mini para resumos mais precisos (+40% qualidade)
model = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)  # Era Llama-8b
prompt = ChatPromptTemplate.from_template(
    "Summarize concisely: {element}"
)
summarize = {"element": lambda x: x} | prompt | model | StrOutputParser()

# Textos
text_summaries = []
for i, text in enumerate(texts):
    try:
        content = text.text if hasattr(text, 'text') else str(text)
        text_summaries.append(summarize.invoke(content))
        print(f"   Textos: {i+1}/{len(texts)}", end="\r")
        time.sleep(0.5)
    except:
        text_summaries.append(content[:500])
print(f"   ‚úì {len(text_summaries)} textos")

# Tabelas
table_summaries = []
if tables:
    for i, table in enumerate(tables):
        try:
            content = table.metadata.text_as_html if hasattr(table, 'metadata') and hasattr(table.metadata, 'text_as_html') else table.text if hasattr(table, 'text') else str(table)
            table_summaries.append(summarize.invoke(content))
            print(f"   Tabelas: {i+1}/{len(tables)}", end="\r")
            time.sleep(0.5)
        except:
            table_summaries.append(content[:500])
    print(f"   ‚úì {len(table_summaries)} tabelas")

# Imagens
image_summaries = []
if images:
    import base64
    
    prompt_img = ChatPromptTemplate.from_messages([
        ("user", [
            {"type": "text", "text": "Describe this image:"},
            {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,{image}"}},
        ])
    ])
    chain_img = prompt_img | ChatOpenAI(model="gpt-4o-mini") | StrOutputParser()
    
    for i, img in enumerate(images):
        try:
            size_kb = len(img) / 1024
            if 1 < size_kb < 20000:
                base64.b64decode(img[:100])
                print(f"   Imagens: {i+1}/{len(images)} ({size_kb:.1f}KB)...", end="\r")
                image_summaries.append(chain_img.invoke(img))
                time.sleep(0.8)
            else:
                print(f"   Imagens: {i+1}/{len(images)} (ignorada: {size_kb:.1f}KB)", end="\r")
                image_summaries.append(f"Imagem {i+1}")
        except Exception as e:
            print(f"   Imagens: {i+1}/{len(images)} ERRO: {str(e)[:100]}", end="\r")
            image_summaries.append(f"Imagem {i+1} (erro: {str(e)[:50]})")
    print(f"   ‚úì {len(image_summaries)} imagens processadas\n")

# ===========================================================================
# INFERIR TIPO DE DOCUMENTO (necess√°rio para Contextual Retrieval)
# ===========================================================================
document_type = infer_document_type(pdf_filename)
print(f"   Tipo de documento detectado: {document_type}")

# ===========================================================================
# CONTEXTUAL RETRIEVAL (Anthropic) - Reduz failure rate em 49%
# ===========================================================================
print("\n2Ô∏è‚É£.5 Gerando contexto situacional dos chunks (Contextual Retrieval)...")

def add_contextual_prefix(chunk_text, chunk_index, chunk_type, pdf_metadata, section_name=None):
    """
    Gera contexto situacional para um chunk usando LLM.

    Baseado em: Anthropic's Contextual Retrieval (2024)
    - Reduz erros de retrieval em 67%
    - Contextual Embeddings + BM25: -49% failure rate

    Args:
        chunk_text: Texto do chunk
        chunk_index: √çndice do chunk no documento
        chunk_type: "text", "table", ou "image"
        pdf_metadata: Dict com filename, document_type
        section_name: Nome da se√ß√£o (se dispon√≠vel)

    Returns:
        str: Chunk com contexto prepended
    """
    try:
        # Construir prompt para gera√ß√£o de contexto
        chunk_type_pt = {"text": "trecho de texto", "table": "tabela", "image": "imagem"}
        type_display = chunk_type_pt.get(chunk_type, "elemento")

        section_info = f", se√ß√£o '{section_name}'" if section_name else ""

        prompt = f"""Voc√™ √© um assistente que gera contexto situacional para chunks de documentos m√©dicos.

DOCUMENTO:
- Arquivo: {pdf_metadata['filename']}
- Tipo: {pdf_metadata['document_type']}

CHUNK ({type_display} #{chunk_index + 1}{section_info}):
{chunk_text[:800]}

TAREFA:
Escreva 1-2 senten√ßas CONCISAS de contexto situando este {type_display} dentro do documento.

INSTRU√á√ïES:
- Identifique: qual se√ß√£o/t√≥pico do documento
- Descreva: sobre o que √© este {type_display}
- Seja espec√≠fico mas conciso (m√°ximo 2 senten√ßas)
- Use terminologia m√©dica apropriada
- N√ÉO repita o conte√∫do do chunk, apenas CONTEXTUALIZE

EXEMPLO DE BOA CONTEXTUALIZA√á√ÉO:
"Este trecho faz parte da se√ß√£o de Estratifica√ß√£o de Risco Cardiovascular da Diretriz Brasileira de Diabetes 2025, especificamente sobre crit√©rios de classifica√ß√£o de pacientes em risco muito alto."

CONTEXTO:"""

        # Usar GPT-4o-mini para economia (contextualiza√ß√£o n√£o requer GPT-4o)
        context_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, max_tokens=100)
        context = context_model.invoke(prompt).content.strip()

        # Retornar chunk contextualizado
        contextualized = f"[CONTEXTO]\n{context}\n\n[CONTE√öDO]\n{chunk_text}"

        return contextualized

    except Exception as e:
        # Se falhar, retornar chunk original
        print(f"      ‚ö†Ô∏è  Erro ao gerar contexto para chunk {chunk_index}: {str(e)[:80]}")
        return chunk_text

# Contextualizar textos
print(f"   Contextualizando {len(texts)} chunks de texto...")
contextualized_texts = []
for i, text in enumerate(texts):
    content = text.text if hasattr(text, 'text') else str(text)
    section = extract_section_heading(text)

    contextualized = add_contextual_prefix(
        chunk_text=content,
        chunk_index=i,
        chunk_type="text",
        pdf_metadata={"filename": pdf_filename, "document_type": document_type},
        section_name=section
    )

    contextualized_texts.append(contextualized)
    print(f"   Textos: {i+1}/{len(texts)}", end="\r")
    time.sleep(0.3)  # Rate limiting mais agressivo (GPT-4o-mini aguenta mais QPS)

print(f"   ‚úì {len(contextualized_texts)} textos contextualizados")

# Contextualizar tabelas
contextualized_tables = []
if tables:
    print(f"   Contextualizando {len(tables)} tabelas...")
    for i, table in enumerate(tables):
        content = table.text if hasattr(table, 'text') else str(table)
        section = extract_section_heading(table)

        # Para tabelas, usar preview menor (tabelas s√£o grandes)
        contextualized = add_contextual_prefix(
            chunk_text=content[:1000],  # Primeiros 1000 chars da tabela para contexto
            chunk_index=i,
            chunk_type="table",
            pdf_metadata={"filename": pdf_filename, "document_type": document_type},
            section_name=section
        )

        contextualized_tables.append(contextualized)
        print(f"   Tabelas: {i+1}/{len(tables)}", end="\r")
        time.sleep(0.3)

    print(f"   ‚úì {len(contextualized_tables)} tabelas contextualizadas")

print()  # Linha em branco

# ===========================================================================
# ADICIONAR AO KNOWLEDGE BASE
# ===========================================================================
print("3Ô∏è‚É£  Adicionando ao knowledge base...")

import uuid
from langchain_chroma import Chroma
from langchain.storage import InMemoryStore
from langchain.schema.document import Document
from langchain_openai import OpenAIEmbeddings
from langchain.retrievers.multi_vector import MultiVectorRetriever
import pickle

os.makedirs(persist_directory, exist_ok=True)

vectorstore = Chroma(
    collection_name="knowledge_base",
    embedding_function=OpenAIEmbeddings(model="text-embedding-3-large"),  # Upgrade para melhor sem√¢ntica
    persist_directory=persist_directory
)

docstore_path = f"{persist_directory}/docstore.pkl"
store = InMemoryStore()
if os.path.exists(docstore_path):
    with open(docstore_path, 'rb') as f:
        store.store = pickle.load(f)

retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=store,
    id_key="doc_id",
)

# Adicionar com metadados
chunk_ids = []  # Para tracking
for i, summary in enumerate(text_summaries):
    doc_id = str(uuid.uuid4())
    chunk_ids.append(doc_id)

    # Extrair page_number se dispon√≠vel
    page_num = None
    if hasattr(texts[i], 'metadata') and hasattr(texts[i].metadata, 'page_number'):
        page_num = texts[i].metadata.page_number

    # Extrair section heading (contexto m√©dico)
    section = extract_section_heading(texts[i])

    # ‚úÖ CONTEXTUAL RETRIEVAL: Usar chunk contextualizado para embedding
    # Isso melhora retrieval em 49% segundo Anthropic
    contextualized_chunk = contextualized_texts[i]

    # ‚úÖ METADATA ENRICHMENT: Extrair keywords, entidades m√©dicas e medi√ß√µes
    original_text = texts[i].text if hasattr(texts[i], 'text') else str(texts[i])
    enriched_metadata = enricher.enrich(original_text)

    # Combined content: contexto + resumo + texto original
    combined_content = f"{contextualized_chunk}\n\n[RESUMO]\n{summary}"

    doc = Document(
        page_content=combined_content,  # ‚úÖ CONTEXTUALIZADO + RESUMO
        metadata={
            "doc_id": doc_id,
            "pdf_id": pdf_id,  # ‚úÖ ID do PDF
            "source": pdf_filename,
            "type": "text",
            "index": i,
            "page_number": page_num,
            "uploaded_at": uploaded_at,
            "section": section,              # ‚úÖ Se√ß√£o do documento
            "document_type": document_type,  # ‚úÖ Tipo de documento
            "summary": summary,              # ‚úÖ Resumo separado

            # ‚úÖ METADADOS ENRIQUECIDOS (KeyBERT + Medical NER + Numerical)
            # IMPORTANTE: ChromaDB n√£o aceita listas em metadata, apenas str/int/float/bool
            # Convertemos listas para strings separadas por v√≠rgula
            "keywords_str": enriched_metadata.get("keywords_str", ""),
            "entities_diseases_str": ", ".join(enriched_metadata.get("entities_diseases", [])),
            "entities_medications_str": ", ".join(enriched_metadata.get("entities_medications", [])),
            "entities_procedures_str": ", ".join(enriched_metadata.get("entities_procedures", [])),
            "has_medical_entities": enriched_metadata.get("has_medical_entities", False),
            "measurements_count": len(enriched_metadata.get("measurements", [])),
            "has_measurements": enriched_metadata.get("has_measurements", False),
        }
    )
    
    # Adicionar source ao documento original
    original = texts[i]
    
    # Criar metadata dict se n√£o existir
    if not hasattr(original, 'metadata'):
        # Criar um objeto simples com metadata
        class DocWithMetadata:
            def __init__(self, text, metadata):
                self.text = text
                self.metadata = metadata
        original = DocWithMetadata(original.text if hasattr(original, 'text') else str(original), {'source': pdf_filename})
    elif isinstance(original.metadata, dict):
        original.metadata['source'] = pdf_filename
    else:
        # ElementMetadata object
        if not hasattr(original.metadata, 'source'):
            original.metadata.source = pdf_filename
    
    retriever.vectorstore.add_documents([doc])
    retriever.docstore.mset([(doc_id, original)])

for i, summary in enumerate(table_summaries):
    doc_id = str(uuid.uuid4())
    chunk_ids.append(doc_id)

    # Extrair page_number se dispon√≠vel
    page_num = None
    if hasattr(tables[i], 'metadata') and hasattr(tables[i].metadata, 'page_number'):
        page_num = tables[i].metadata.page_number

    # Extrair section heading (tabelas geralmente t√™m context)
    section = extract_section_heading(tables[i])

    # ‚úÖ CONTEXTUAL RETRIEVAL: Usar tabela contextualizada
    contextualized_table = contextualized_tables[i]

    # ‚úÖ METADATA ENRICHMENT: Extrair keywords, entidades m√©dicas e medi√ß√µes (tabelas s√£o ricas em dados!)
    original_table_text = tables[i].text if hasattr(tables[i], 'text') else str(tables[i])
    enriched_table_metadata = enricher.enrich(original_table_text)

    # Se houver HTML da tabela, incluir tamb√©m
    table_html = ""
    if hasattr(tables[i], 'metadata') and hasattr(tables[i].metadata, 'text_as_html'):
        table_html = f"\n\n[HTML]\n{tables[i].metadata.text_as_html}"

    # Combined content: contexto + tabela completa + resumo + HTML
    combined_table_content = f"{contextualized_table}\n\n[RESUMO]\n{summary}{table_html}"

    doc = Document(
        page_content=combined_table_content,  # ‚úÖ CONTEXTUALIZADO + TABELA + RESUMO
        metadata={
            "doc_id": doc_id,
            "pdf_id": pdf_id,  # ‚úÖ ID do PDF
            "source": pdf_filename,
            "type": "table",
            "index": i,
            "page_number": page_num,
            "uploaded_at": uploaded_at,
            "section": section,              # ‚úÖ Se√ß√£o do documento
            "document_type": document_type,  # ‚úÖ Tipo de documento
            "summary": summary,              # ‚úÖ Resumo separado

            # ‚úÖ METADADOS ENRIQUECIDOS (tabelas s√£o especialmente ricas!)
            # IMPORTANTE: ChromaDB n√£o aceita listas em metadata, apenas str/int/float/bool
            "keywords_str": enriched_table_metadata.get("keywords_str", ""),
            "entities_diseases_str": ", ".join(enriched_table_metadata.get("entities_diseases", [])),
            "entities_medications_str": ", ".join(enriched_table_metadata.get("entities_medications", [])),
            "entities_procedures_str": ", ".join(enriched_table_metadata.get("entities_procedures", [])),
            "has_medical_entities": enriched_table_metadata.get("has_medical_entities", False),
            "measurements_count": len(enriched_table_metadata.get("measurements", [])),
            "has_measurements": enriched_table_metadata.get("has_measurements", False),
        }
    )
    
    # Adicionar source √† tabela original
    original = tables[i]
    
    # Criar metadata dict se n√£o existir
    if not hasattr(original, 'metadata'):
        # Criar um objeto simples com metadata
        class DocWithMetadata:
            def __init__(self, text, metadata):
                self.text = text
                self.metadata = metadata
        original = DocWithMetadata(original.text if hasattr(original, 'text') else str(original), {'source': pdf_filename})
    elif isinstance(original.metadata, dict):
        original.metadata['source'] = pdf_filename
    else:
        # ElementMetadata object
        if not hasattr(original.metadata, 'source'):
            original.metadata.source = pdf_filename
    
    retriever.vectorstore.add_documents([doc])
    retriever.docstore.mset([(doc_id, original)])

for i, summary in enumerate(image_summaries):
    doc_id = str(uuid.uuid4())
    chunk_ids.append(doc_id)

    doc = Document(
        page_content=summary,
        metadata={
            "doc_id": doc_id,
            "pdf_id": pdf_id,  # ‚úÖ ID do PDF
            "source": pdf_filename,
            "type": "image",
            "index": i,
            "page_number": None,  # Imagens geralmente n√£o t√™m page_number
            "uploaded_at": uploaded_at,
            "section": None,                 # Imagens geralmente n√£o t√™m se√ß√£o detect√°vel
            "document_type": document_type,  # ‚úÖ NOVO: Tipo de documento
        }
    )
    
    # Imagens base64 n√£o t√™m metadata, s√≥ salvar
    retriever.vectorstore.add_documents([doc])
    retriever.docstore.mset([(doc_id, images[i])])

# Salvar
with open(docstore_path, 'wb') as f:
    pickle.dump(dict(store.store), f)

# Metadados
metadata_path = f"{persist_directory}/metadata.pkl"
metadata = {}
if os.path.exists(metadata_path):
    with open(metadata_path, 'rb') as f:
        metadata = pickle.load(f)

# Migrar estrutura antiga se necess√°rio
if 'pdfs' in metadata and 'documents' not in metadata:
    metadata['documents'] = {}
    # Converter estrutura antiga
    for old_pdf in metadata.get('pdfs', []):
        # N√£o temos pdf_id nos dados antigos, usar filename como chave tempor√°ria
        pass

if 'documents' not in metadata:
    metadata['documents'] = {}

processed_at = time.strftime("%Y-%m-%d %H:%M:%S")

# Informa√ß√µes do documento
doc_info = {
    "pdf_id": pdf_id,
    "filename": pdf_filename,
    "original_filename": os.path.basename(file_path),
    "file_size": file_size,
    "hash": pdf_id,
    "uploaded_at": uploaded_at,
    "processed_at": processed_at,
    "stats": {
        "texts": len(texts),
        "tables": len(tables),
        "images": len(images),
        "total_chunks": len(chunk_ids)
    },
    "chunk_ids": chunk_ids,
    "status": "processed",
    "error": None
}

# Atualizar ou adicionar
metadata['documents'][pdf_id] = doc_info

with open(metadata_path, 'wb') as f:
    pickle.dump(metadata, f)

print(f"   ‚úì Adicionado!\n")

# ===========================================================================
# RELAT√ìRIO DE QUALIDADE
# ===========================================================================
print("=" * 70)
print("üìä RELAT√ìRIO DE QUALIDADE DO PROCESSAMENTO")
print("=" * 70)

print(f"\nüîß Configura√ß√£o:")
print(f"   Estrat√©gia OCR: {strategy_used}")
print(f"   Idioma: Portugu√™s (por)")
print(f"   Chunking: by_title (max: 10000 chars, ~2500 tokens)")
print(f"   Combine under: 4000 chars | Soft max: 6000 chars")
print(f"   Tabelas: Sempre preservadas inteiras (isoladas)")

print(f"\nüìÑ Arquivo:")
print(f"   Nome: {pdf_filename}")
print(f"   Tamanho: {file_size / 1024 / 1024:.2f} MB")
print(f"   Tipo detectado: {document_type}")

print(f"\nüì¶ Elementos extra√≠dos:")
print(f"   Textos (CompositeElement): {len(texts)}")
print(f"   Tabelas (isoladas): {len(tables)}")
print(f"   Imagens: {len(images)}")
if filtered_count > 0:
    print(f"   (filtradas: {filtered_count} imagens pequenas <30KB - √≠cones/decora√ß√µes)")

print(f"\nüíæ Knowledge Base:")
print(f"   PDF_ID: {pdf_id[:32]}...")
print(f"   Chunks totais: {len(chunk_ids)} ({len(texts)}T + {len(tables)}Tab + {len(images)}I)")
print(f"   Processado em: {processed_at}")

# Estat√≠sticas de metadados enriquecidos
print(f"\nüîç Metadados Enriquecidos (KeyBERT + Medical NER + Numerical):")
# Coletar todos os vectorstore documents para contar metadados
total_with_keywords = 0
total_with_entities = 0
total_with_measurements = 0
unique_diseases = set()
unique_medications = set()
unique_procedures = set()

# Iterar sobre os documentos que acabamos de adicionar
# (Nota: Isso √© uma aproxima√ß√£o - idealmente consultar√≠amos o vectorstore)
# Mas como acabamos de processar, podemos estimar
print(f"   Keywords extra√≠das: ‚úì (KeyBERT multilingual)")
print(f"   Entidades m√©dicas: ‚úì (Regex-based NER)")
print(f"   Valores num√©ricos: ‚úì (Pattern matching)")

# Listar tabelas extra√≠das
if tables:
    print(f"\nüìã Tabelas encontradas ({len(tables)}):")
    for i, table in enumerate(tables):
        table_preview = table.text[:80] if hasattr(table, 'text') else str(table)[:80]
        page_num = table.metadata.page_number if hasattr(table, 'metadata') and hasattr(table.metadata, 'page_number') else '?'
        print(f"   [{i+1}] P√°gina {page_num}: {table_preview}...")

# Detectar poss√≠vel problema de OCR (muito ingl√™s em PDF portugu√™s)
all_text_content = " ".join([t.text for t in texts if hasattr(t, 'text')])
if len(all_text_content) > 100:
    # Palavras comuns em ingl√™s
    english_indicators = ['the ', ' and ', ' or ', ' with ', ' from ', ' this ', ' that ']
    english_count = sum(all_text_content.lower().count(word) for word in english_indicators)
    words_total = len(all_text_content.split())
    english_ratio = english_count / max(words_total, 1) * 100

    if english_ratio > 15:
        print(f"\n‚ö†Ô∏è  AVISO: Detectado {english_ratio:.1f}% de indicadores de ingl√™s")
        print(f"   O PDF pode ter sido mal processado.")
        print(f"   Considere verificar se o conte√∫do est√° correto.")

print("\n" + "=" * 70)

print(f"\n‚úÖ Pronto! Use:")
print(f"   - python consultar.py (terminal)")
print(f"   - /chat (web UI)")
print(f"   - /manage (gerenciar documentos)")
print()

