# üîç AN√ÅLISE CR√çTICA: Pipeline de Processamento de PDFs

## ‚ùå PROBLEMAS IDENTIFICADOS

### 1. **TABELAS QUEBRADAS PELO CHUNKING** (CR√çTICO)
**Arquivo:** `adicionar_pdf.py` linhas 86-90

```python
chunking_strategy="by_title",
max_characters=10000,
combine_text_under_n_chars=2000,
new_after_n_chars=6000,
```

**Problema:**
- `chunking_strategy="by_title"` pode quebrar tabelas no meio
- Se uma tabela tem m√∫ltiplas se√ß√µes (Low, Intermediate, Alto, Muito Alto), cada se√ß√£o pode virar um chunk separado
- Isso explica por que encontramos apenas "Low Intermediate" na tabela extra√≠da

**Evid√™ncia:**
- `/search-table` retornou apenas 1 tabela com conte√∫do parcial em ingl√™s
- Keywords "3 ou mais fatores" e "Hipercolesterolemia Familiar" n√£o existem em nenhum chunk
- Tabela foi processada como tipo "table" mas conte√∫do incompleto

**Impacto:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê CR√çTICO
- Respostas incompletas para perguntas sobre crit√©rios de risco
- Informa√ß√£o cr√≠tica perdida permanentemente


### 2. **OCR SEM CONFIGURA√á√ÉO DE IDIOMA**
**Arquivo:** `adicionar_pdf.py` linha 80-84

```python
return partition_pdf(
    filename=file_path,
    infer_table_structure=True,
    strategy=strategy,
    # ‚ùå FALTA: languages=["por"]
```

**Problema:**
- Unstructured usa Tesseract OCR por padr√£o
- Sem `languages=["por"]`, o OCR detecta automaticamente o idioma
- Tabelas com formata√ß√£o complexa podem ser mal interpretadas como ingl√™s

**Evid√™ncia:**
- Tabela extra√≠da tem conte√∫do em ingl√™s: "Low Intermediate"
- Texto original est√° em portugu√™s

**Impacto:** ‚≠ê‚≠ê‚≠ê‚≠ê ALTO
- OCR impreciso reduz qualidade do retrieval
- Embeddings de texto em ingl√™s n√£o casam com queries em portugu√™s


### 3. **RESUMOS PERDEM INFORMA√á√ÉO DETALHADA** (CR√çTICO)
**Arquivo:** `adicionar_pdf.py` linhas 345-377

```python
# Gerar resumos com GPT-4o-mini
summarize = {"element": lambda x: x} | prompt | model | StrOutputParser()

# PROBLEMA: Resumo √© armazenado no vectorstore, texto original no docstore
```

**Problema:**
- **Busca √© feita APENAS nos resumos** (vectorstore)
- Resumos podem omitir detalhes espec√≠ficos como "3 ou mais fatores de risco"
- GPT-4o-mini com prompt "Summarize concisely" pode generalizar demais

**Exemplo do que pode acontecer:**
```
Texto original: "MUITO ALTO: Hipercolesterolemia Familiar, 3 ou mais fatores de risco, albumin√∫ria >300mg/g..."

Resumo gerado: "Crit√©rios de classifica√ß√£o de risco cardiovascular incluem fatores como hipercolesterolemia, albumin√∫ria e fun√ß√£o renal"

‚ùå PERDEU: "3 ou mais fatores", "Hipercolesterolemia Familiar", valores espec√≠ficos
```

**Impacto:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê CR√çTICO
- Keywords espec√≠ficos n√£o aparecem nos embeddings
- Queries espec√≠ficas n√£o encontram documentos relevantes
- Cohere reranker n√£o consegue compensar se o doc n√£o est√° nos top 30


### 4. **FILTRO DE IMAGENS PODE REMOVER TABELAS**
**Arquivo:** `adicionar_pdf.py` linhas 278, 286-297

```python
MIN_IMAGE_SIZE_KB = float(os.getenv("MIN_IMAGE_SIZE_KB", "5"))

# Filtrar imagens pequenas
if size_kb >= MIN_IMAGE_SIZE_KB:
    # ... adicionar
else:
    filtered_count += 1
```

**Problema:**
- Tabelas extra√≠das como imagens podem ter <5KB se forem simples
- Tabelas compactas (poucas linhas) podem ser descartadas
- TABELA 1 de risco cardiovascular pode ser compacta

**Impacto:** ‚≠ê‚≠ê‚≠ê M√âDIO
- Tabelas importantes podem ser removidas silenciosamente
- Sem warning no log


### 5. **ESTRAT√âGIA `hi_res` vs `fast`**
**Arquivo:** `adicionar_pdf.py` linhas 76-101

```python
strategy_env = os.getenv("UNSTRUCTURED_STRATEGY", "hi_res").strip().lower()

try:
    chunks = run_partition(strategy_env)
except Exception as e:
    # Fallback para 'fast'
    if "libGL.so.1" in str(e):
        chunks = run_partition("fast")
```

**Problema:**
- `hi_res`: Melhor OCR, mas pode falhar em ambiente Railway (sem libGL)
- `fast`: Mais r√°pido, mas OCR inferior, n√£o detecta tabelas complexas bem
- Railway provavelmente usa `fast` por falta de libGL

**Evid√™ncia:**
- Log de upload n√£o mostra qual estrat√©gia foi usada
- Tabela em ingl√™s sugere OCR ruim

**Impacto:** ‚≠ê‚≠ê‚≠ê‚≠ê ALTO
- Qualidade de extra√ß√£o muito inferior com `fast`
- Usu√°rio n√£o sabe qual estrat√©gia foi usada


### 6. **CHUNKING AGRESSIVO DEMAIS**
**Arquivo:** `adicionar_pdf.py` linha 89

```python
new_after_n_chars=6000,  # For√ßa quebra a cada 6000 chars
```

**Problema:**
- For√ßa quebra a cada 6000 caracteres
- Tabelas longas ou se√ß√µes m√©dicas detalhadas s√£o quebradas
- Contexto √© perdido entre chunks

**Impacto:** ‚≠ê‚≠ê‚≠ê M√âDIO
- Informa√ß√µes relacionadas ficam em chunks separados
- LLM precisa conectar m√∫ltiplos chunks (nem sempre funciona)


### 7. **SEPARA√á√ÉO DE ELEMENTOS INCOMPLETA**
**Arquivo:** `adicionar_pdf.py` linhas 108-121

```python
for chunk in chunks:
    chunk_type = str(type(chunk).__name__)

    if "Table" in chunk_type and chunk not in tables:
        tables.append(chunk)
    elif chunk_type in ['CompositeElement', 'NarrativeText', 'Title', 'Text', 'ListItem']:
        texts.append(chunk)
```

**Problema:**
- Lista manual de tipos pode perder novos elementos da Unstructured
- Elementos n√£o classificados s√£o ignorados silenciosamente
- N√£o h√° log de elementos descartados

**Impacto:** ‚≠ê‚≠ê BAIXO
- Alguns elementos podem ser perdidos sem aviso


### 8. **SEM VALIDA√á√ÉO DE QUALIDADE P√ìS-PROCESSAMENTO**

**Problema:**
- N√£o h√° verifica√ß√£o se tabelas cr√≠ticas foram extra√≠das
- N√£o h√° compara√ß√£o de # de p√°ginas vs # de chunks
- N√£o h√° detec√ß√£o de OCR falho (muito texto em ingl√™s em PDF portugu√™s)

**Impacto:** ‚≠ê‚≠ê‚≠ê M√âDIO
- Problemas s√≥ s√£o detectados quando usu√°rio pergunta


## ‚úÖ SOLU√á√ïES PROPOSTAS

### SOLU√á√ÉO 1: Desabilitar chunking para preservar tabelas completas

```python
# ANTES (adicionar_pdf.py linha 86):
chunks = partition_pdf(
    filename=file_path,
    infer_table_structure=True,
    strategy=strategy,
    extract_image_block_types=["Image", "Table"],
    extract_image_block_to_payload=True,
    chunking_strategy="by_title",  # ‚ùå QUEBRA TABELAS
    max_characters=10000,
    combine_text_under_n_chars=2000,
    new_after_n_chars=6000,
)

# DEPOIS:
chunks = partition_pdf(
    filename=file_path,
    infer_table_structure=True,
    strategy=strategy,
    extract_image_block_types=["Image", "Table"],
    extract_image_block_to_payload=True,
    languages=["por"],  # ‚úÖ FOR√áA PORTUGU√äS
    # ‚úÖ REMOVER chunking autom√°tico - fazer manual depois
)
```

**Benef√≠cios:**
- Tabelas preservadas inteiras
- OCR em portugu√™s correto
- Controle total sobre chunking

**Trade-off:**
- Chunks maiores ‚Üí mais tokens no contexto
- Solu√ß√£o: fazer chunking manual AP√ìS separar tabelas


### SOLU√á√ÉO 2: Armazenar texto original + resumo nos embeddings

```python
# ANTES: S√≥ resumo no vectorstore
doc = Document(
    page_content=summary,  # ‚ùå S√ì RESUMO
    metadata={"doc_id": doc_id, ...}
)

# DEPOIS: Texto original + resumo
doc = Document(
    page_content=f"{summary}\n\n---ORIGINAL---\n{original_text}",  # ‚úÖ AMBOS
    metadata={"doc_id": doc_id, "summary": summary, ...}
)
```

**Benef√≠cios:**
- Keywords espec√≠ficos aparecem nos embeddings
- Retrieval encontra texto mesmo se resumo omitiu
- Cohere rerank v√™ texto completo

**Trade-off:**
- Embeddings maiores (mais custo)
- Solu√ß√£o: aceitar custo para melhor qualidade


### SOLU√á√ÉO 3: Estrat√©gia h√≠brida para tabelas

```python
# Processar tabelas separadamente sem resumo
for table in tables:
    # Armazenar tabela COMPLETA sem resumir
    doc = Document(
        page_content=table.text,  # ‚úÖ TEXTO COMPLETO
        metadata={"doc_id": doc_id, "type": "table", ...}
    )

    # Tamb√©m adicionar imagem da tabela
    if hasattr(table.metadata, 'image_base64'):
        # Armazenar imagem tamb√©m
        ...
```

**Benef√≠cios:**
- Tabelas preservadas 100%
- Retrieval encontra keywords espec√≠ficos
- Multimodal: texto + imagem da tabela

**Trade-off:**
- Mais chunks (41 ‚Üí ~60)
- Solu√ß√£o: aumentar `k` no retrieval


### SOLU√á√ÉO 4: Logging detalhado do processamento

```python
# Adicionar ao final do processamento:
print("\nüìä RELAT√ìRIO DE QUALIDADE:")
print(f"   Estrat√©gia OCR: {strategy_used}")
print(f"   P√°ginas no PDF: {num_pages}")
print(f"   Chunks gerados: {len(chunk_ids)}")
print(f"   Tabelas encontradas: {len(tables)}")

# Listar tabelas extra√≠das
for i, table in enumerate(tables):
    preview = table.text[:100] if hasattr(table, 'text') else str(table)[:100]
    print(f"   Tabela {i+1}: {preview}...")

# Detectar OCR ruim (muito ingl√™s em PDF portugu√™s)
all_text = " ".join([t.text for t in texts if hasattr(t, 'text')])
english_words = len([w for w in all_text.split() if w.lower() in ['the', 'and', 'or', 'with']])
total_words = len(all_text.split())
if english_words / total_words > 0.1:
    print(f"   ‚ö†Ô∏è  AVISO: {english_words/total_words*100:.1f}% de palavras em ingl√™s detectadas")
    print(f"      PDF pode ter sido mal processado. Considere reprocessar com OCR otimizado.")
```


### SOLU√á√ÉO 5: Valida√ß√£o p√≥s-processamento

```python
# Verificar se keywords cr√≠ticos foram capturados
def validate_extraction(texts, tables, critical_keywords):
    """Verifica se keywords cr√≠ticos foram extra√≠dos"""
    all_content = " ".join([
        t.text.lower() for t in texts if hasattr(t, 'text')
    ] + [
        t.text.lower() for t in tables if hasattr(t, 'text')
    ])

    missing = []
    for keyword in critical_keywords:
        if keyword.lower() not in all_content:
            missing.append(keyword)

    if missing:
        print(f"\n   ‚ö†Ô∏è  KEYWORDS AUSENTES: {missing}")
        print(f"      Processamento pode estar incompleto")

    return len(missing) == 0

# Para PDFs de diabetes, verificar:
if 'diabetes' in pdf_filename.lower():
    validate_extraction(texts, tables, [
        "hipercolesterolemia familiar",
        "muito alto",
        "albumin√∫ria",
        "TFG"
    ])
```


## üéØ PRIORIZA√á√ÉO DAS SOLU√á√ïES

| Solu√ß√£o | Impacto | Esfor√ßo | Prioridade |
|---------|---------|---------|------------|
| 1. Desabilitar chunking autom√°tico | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Baixo | **P0 - CR√çTICO** |
| 2. Texto original + resumo | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | M√©dio | **P0 - CR√çTICO** |
| 3. Estrat√©gia h√≠brida tabelas | ‚≠ê‚≠ê‚≠ê‚≠ê | M√©dio | **P1 - ALTO** |
| 4. Logging detalhado | ‚≠ê‚≠ê‚≠ê | Baixo | **P1 - ALTO** |
| 5. Valida√ß√£o p√≥s-processamento | ‚≠ê‚≠ê‚≠ê | M√©dio | **P2 - M√âDIO** |


## üìã PLANO DE IMPLEMENTA√á√ÉO

### Fase 1: Corre√ß√µes Cr√≠ticas (30 min)
1. ‚úÖ Remover `chunking_strategy="by_title"`
2. ‚úÖ Adicionar `languages=["por"]`
3. ‚úÖ Armazenar texto original + resumo nos embeddings

### Fase 2: Melhorias Tabelas (20 min)
4. ‚úÖ Processar tabelas sem resumo (texto completo)
5. ‚úÖ Logging de tabelas extra√≠das

### Fase 3: Valida√ß√£o (10 min)
6. ‚úÖ Adicionar relat√≥rio de qualidade
7. ‚úÖ Valida√ß√£o de keywords cr√≠ticos


## üîç TESTE AP√ìS IMPLEMENTA√á√ÉO

1. Reprocessar PDF de diabetes
2. Verificar `/search-table` retorna `has_tabela_1: true`
3. Fazer query: "Quais s√£o todos os crit√©rios de muito alto risco cardiovascular?"
4. Verificar resposta inclui:
   - ‚úÖ Hipercolesterolemia Familiar
   - ‚úÖ 3 ou mais fatores de risco
   - ‚úÖ Albumin√∫ria >300
   - ‚úÖ TFG <30
   - ‚úÖ Todos outros crit√©rios da TABELA 1
