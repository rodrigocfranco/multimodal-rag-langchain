# An√°lise dos Testes Desafiadores e Solu√ß√µes

## üìä Resultados dos Testes

### ‚úÖ Sucesso (50% - 3/6 perguntas)

**Q2: Contraindica√ß√µes da metformina** ‚úÖ
- Retornou lista completa e precisa
- Observa√ß√£o: N√£o separou "absolutas" vs "relativas", mas trouxe todas as informa√ß√µes

**Q5: Valor EXATO de TFG** ‚úÖ
- Resposta perfeita: "<30 ml/min/1.73m2"
- Precis√£o num√©rica mantida

**Q6: Valores de HbA1c** ‚úÖ
- 9 valores listados com contextos corretos
- **Sistema funcionou MUITO BEM aqui!**

---

### ‚ùå Falhas (50% - 3/6 perguntas)

**Q1: Rela√ß√£o albumin√∫ria e risco CV** ‚ùå
```
Resposta: "A informa√ß√£o solicitada n√£o est√° presente nos documentos fornecidos"
```

**Causa:** Pergunta abstrata com palavra-chave vaga ("rela√ß√£o")
- Informa√ß√£o EST√Å no documento ("albumin√∫ria >300mg/g" como crit√©rio de muito alto risco)
- Reranker filtrou porque chunk lista crit√©rio, mas n√£o explica "rela√ß√£o"

---

**Q3: Quando N√ÉO usar insulina como primeira linha** ‚ùå
```
Resposta: "A informa√ß√£o solicitada n√£o est√° presente"
```

**Causa:** Nega√ß√£o + informa√ß√£o impl√≠cita
- Documento diz: "Com HbA1c < 7,5% usar iSGLT2" (implica: N√ÉO use insulina)
- Retrieval n√£o captura l√≥gica inversa

---

**Q4: Glicose jejum normal n√£o descarta diabetes?** ‚ùå
```
Resposta: "A informa√ß√£o solicitada n√£o est√° presente"
```

**Causa:** Dupla nega√ß√£o
- Embedding fraco com: "normal N√ÉO descarta"
- Informa√ß√£o provavelmente est√° (TOTG, HbA1c como alternativas)

---

## üîç Diagn√≥stico T√©cnico

### Problema 1: Embeddings Ruins com Nega√ß√µes/Abstra√ß√µes

**Embeddings (text-embedding-3-large) s√£o FRACOS em:**
- ‚ùå Nega√ß√µes: "N√ÉO usar"
- ‚ùå Abstra√ß√µes: "rela√ß√£o entre X e Y"
- ‚ùå Dupla nega√ß√£o: "normal N√ÉO descarta"
- ‚ùå L√≥gica inversa: Query pede "quando N√ÉO X", documento diz "quando Y"

**Exemplo do problema:**
```
Query: "Qual a rela√ß√£o entre albumin√∫ria e risco cardiovascular?"
Embedding: representa "albumin√∫ria" + "risco" + "rela√ß√£o" (vago)

Chunk: "Crit√©rios de muito alto risco: albumin√∫ria >300mg/g"
Embedding: representa "albumin√∫ria" + "risco" + "crit√©rios" (espec√≠fico)

Cosine similarity: 0.85 (bom!)
Mas Cohere Reranker v√™:
  - Query pede "rela√ß√£o" (explica√ß√£o)
  - Chunk lista "crit√©rios" (fato)
  ‚Üí Score: 0.3 (baixo) ‚Üí FILTRADO!
```

---

### Problema 2: Chunking Fragmenta Contexto

**Cen√°rio:**

**Chunk 1:**
```
"Crit√©rios de muito alto risco cardiovascular: albumin√∫ria >300mg/g"
```

**Chunk 2 (em outra parte do documento):**
```
"A presen√ßa de albumin√∫ria √© um marcador de les√£o endotelial
e est√° associada a aumento de eventos cardiovasculares"
```

**O que acontece:**
- Query: "Qual a rela√ß√£o entre albumin√∫ria e risco CV?"
- **Chunk 2 seria PERFEITO** (explica a rela√ß√£o!)
- **Mas Chunk 1 tem score maior** (mais keywords)
- **Reranker prioriza Chunk 1** (denso, mas n√£o explica)
- **LLM recebe Chunk 1**, que n√£o responde "rela√ß√£o"

---

### Problema 3: top_n=5 Muito Restritivo

**Cen√°rio:**
- Busca inicial: 20 chunks
- Cohere ranqueia e retorna **TOP 5**
- Se informa√ß√£o est√° no **6¬∫ melhor**, √© PERDIDA

**Para perguntas complexas, precisa de MAIS contexto.**

---

## üõ†Ô∏è Solu√ß√µes Implementadas

### ‚úÖ Solu√ß√£o 1: Aumentar top_n (5 ‚Üí 8)

**Arquivo:** `consultar_com_rerank.py`

**Mudan√ßa:**
```python
# ANTES
compressor = CohereRerank(
    model="rerank-multilingual-v3.0",
    top_n=5  # Apenas 5 documentos
)

# DEPOIS (‚úÖ IMPLEMENTADO)
compressor = CohereRerank(
    model="rerank-multilingual-v3.0",
    top_n=8  # ‚úÖ Aumentado para 8 (60% mais contexto)
)
```

**Por qu√™ ajuda:**
- Mais chunks chegam ao LLM
- Maior chance de pegar chunk com "explica√ß√£o" da rela√ß√£o (Chunk 2)
- N√£o prejudica muito a lat√™ncia (Cohere √© r√°pido)

**Trade-off:**
- ‚úÖ Melhor recall (menos "informa√ß√£o n√£o presente")
- ‚ö†Ô∏è Custo Cohere aumenta ~60% (mas ainda barato: $1/1000 queries)
- ‚ö†Ô∏è Token count do LLM aumenta ~60% (GPT-4o-mini √© barato)

---

### üöÄ Solu√ß√£o 2: Melhorar Prompt com "Infer√™ncia Moderada"

**Status:** PROPOSTA (n√£o implementada ainda)

**Problema atual:**
Prompt pro√≠be qualquer infer√™ncia:
```
"Responda APENAS com informa√ß√µes EXPLICITAMENTE no contexto"
```

Isso √© **BOM** para evitar alucina√ß√£o, mas **RUIM** para:
- Perguntas sobre "rela√ß√£o" (precisa conectar chunks)
- Nega√ß√µes impl√≠citas ("n√£o usar X" quando documento diz "usar Y")

**Solu√ß√£o:**
Adicionar regra para **infer√™ncia moderada**:

```python
system_instruction = """Voc√™ √© um assistente de pesquisa m√©dica RIGOROSO.

REGRAS CR√çTICAS:
1. Responda APENAS com informa√ß√µes que est√£o no contexto fornecido
2. Se a informa√ß√£o N√ÉO estiver no contexto, responda: "A informa√ß√£o solicitada n√£o est√° presente nos documentos fornecidos"
3. NUNCA use conhecimento geral ou externo
4. Cite EXATAMENTE como est√° escrito no documento
5. Se houver listas, tabelas ou crit√©rios, reproduza-os FIELMENTE
6. Mantenha formata√ß√£o original (bullets, n√∫meros, etc)

üÜï INFER√äNCIAS PERMITIDAS:
7. Se a pergunta pede "rela√ß√£o entre X e Y", voc√™ PODE conectar informa√ß√µes
   de DIFERENTES chunks do contexto, desde que cite ambos
8. Se a pergunta pede "quando N√ÉO fazer X" e o documento diz "fazer Y em situa√ß√£o Z",
   voc√™ PODE inferir que "N√ÉO fazer X quando n√£o estiver na situa√ß√£o Z"
9. SEMPRE cite EXATAMENTE os trechos que usou para fazer a infer√™ncia

CONTEXTO DOS DOCUMENTOS:
{context}

PERGUNTA DO USU√ÅRIO:
{question}

RESPOSTA (baseada SOMENTE no contexto acima):"""
```

**Exemplo de uso:**
```
Query: "Qual a rela√ß√£o entre albumin√∫ria e risco cardiovascular?"

Chunks recebidos:
[1] "Crit√©rios de muito alto risco CV: albumin√∫ria >300mg/g"
[2] "Albumin√∫ria √© marcador de les√£o endotelial associada a eventos CV"

Resposta:
"Segundo o documento, a albumin√∫ria est√° relacionada ao risco
cardiovascular de duas formas:

1. √â um CRIT√âRIO de muito alto risco cardiovascular quando >300mg/g [Fonte: chunk 1]
2. √â um MARCADOR de les√£o endotelial associada a aumento de eventos
   cardiovasculares [Fonte: chunk 2]

Portanto, a presen√ßa de albumin√∫ria (especialmente >300mg/g) indica
maior risco cardiovascular."
```

**Trade-off:**
- ‚úÖ Responde perguntas abstratas/relacionais
- ‚úÖ Lida com nega√ß√µes impl√≠citas
- ‚ö†Ô∏è **RISCO:** LLM pode "inferir demais" e alucinar
- ‚ö†Ô∏è Precisa testar cuidadosamente

**Recomenda√ß√£o:** Testar com `top_n=8` primeiro. Se ainda falhar Q1/Q3/Q4, implementar esta solu√ß√£o.

---

### üî¨ Solu√ß√£o 3: Query Expansion (Avan√ßado)

**Status:** PROPOSTA (n√£o implementada)

**Como funciona:**
Antes de fazer embedding search, expandir query para incluir sin√¥nimos/reformula√ß√µes:

```python
def expand_query(question):
    """Usa GPT-4o-mini para gerar queries alternativas"""
    prompt = f"""
Dada esta pergunta m√©dica:
"{question}"

Gere 3 reformula√ß√µes que preservem o significado mas usem palavras diferentes.

Exemplos:
- "rela√ß√£o entre X e Y" ‚Üí "como X afeta Y", "associa√ß√£o entre X e Y", "X est√° relacionado a Y"
- "quando N√ÉO usar X" ‚Üí "contraindica√ß√µes de X", "situa√ß√µes onde X √© inadequado"

Retorne apenas as 3 reformula√ß√µes, uma por linha.
"""
    # Chamar GPT-4o-mini (barato)
    expansions = gpt_mini.invoke(prompt).split('\n')
    return [question] + expansions  # Query original + 3 varia√ß√µes
```

Ent√£o buscar com TODAS as varia√ß√µes:
```python
queries = expand_query("Qual a rela√ß√£o entre albumin√∫ria e risco CV?")
# queries = [
#   "Qual a rela√ß√£o entre albumin√∫ria e risco CV?",
#   "Como albumin√∫ria afeta risco cardiovascular?",
#   "Associa√ß√£o entre albumin√∫ria e eventos cardiovasculares",
#   "Albumin√∫ria est√° relacionada a risco CV?"
# ]

all_docs = []
for q in queries:
    docs = vectorstore.similarity_search(q, k=5)  # 5 docs por query
    all_docs.extend(docs)

# Remover duplicatas
unique_docs = remove_duplicates(all_docs)  # ~15-18 docs √∫nicos

# Rerank todos
reranked = cohere.rerank(unique_docs, original_question, top_n=8)
```

**Por qu√™ ajuda:**
- Query "associa√ß√£o entre" pode encontrar Chunk 2 (que usa "associada")
- Query "contraindica√ß√µes" encontra chunks de nega√ß√£o

**Trade-off:**
- ‚úÖ Recall MUITO melhor
- ‚ùå 4x mais chamadas ao vectorstore (lat√™ncia +300ms)
- ‚ùå Mais custos de Cohere rerank
- ‚ùå Complexidade de implementa√ß√£o

**Recomenda√ß√£o:** Implementar APENAS se Solu√ß√£o 1+2 falharem.

---

## üìà Roadmap de Melhorias

### Fase 1: Quick Win (‚úÖ FEITO)
- [x] Aumentar `top_n` de 5 ‚Üí 8
- [ ] Testar com queries Q1, Q3, Q4
- [ ] Validar que n√£o piorou Q2, Q5, Q6

**Expectativa:** Resolver 1-2 das 3 falhas (provavelmente Q1 e Q4)

---

### Fase 2: Prompt Engineering (Se Fase 1 n√£o resolver tudo)
- [ ] Implementar regras de "infer√™ncia moderada"
- [ ] Testar com 10 perguntas (as 6 atuais + 4 novas)
- [ ] Validar que n√£o h√° alucina√ß√£o

**Expectativa:** Resolver Q3 (nega√ß√µes impl√≠citas)

---

### Fase 3: Query Expansion (Apenas se necess√°rio)
- [ ] Implementar expand_query()
- [ ] Testar lat√™ncia (<3s aceit√°vel)
- [ ] Validar custo-benef√≠cio

**Expectativa:** Resolver 100% das queries (mas pode ser overkill)

---

## üß™ Como Testar as Melhorias

### Teste 1: Verificar se top_n=8 melhorou

**Usar /debug-retrieval:**
```
Query: "Qual a rela√ß√£o entre albumin√∫ria e risco cardiovascular?"

ANTES (top_n=5):
  reranked.count: 5
  Chunks: [...]

DEPOIS (top_n=8):
  reranked.count: 8
  Chunks: [...]  ‚Üê Verificar se Chunk 2 apareceu!
```

---

### Teste 2: Refazer perguntas falhadas

**Q1:** "Qual a rela√ß√£o entre albumin√∫ria e risco cardiovascular?"
- ‚úÖ **Sucesso:** Retorna explica√ß√£o conectando crit√©rios + marcador
- ‚ùå **Falha:** Ainda diz "n√£o est√° presente"

**Q3:** "Em quais situa√ß√µes a diretriz recomenda N√ÉO usar insulina como primeira linha?"
- ‚úÖ **Sucesso:** Retorna "HbA1c < 7,5%, usar iSGLT2 em monoterapia"
- ‚ùå **Falha:** Ainda diz "n√£o est√° presente"

**Q4:** "Existem situa√ß√µes onde glicose em jejum normal N√ÉO descarta diabetes?"
- ‚úÖ **Sucesso:** Retorna "Sim, diabetes pode ser diagnosticado por HbA1c ou TOTG"
- ‚ùå **Falha:** Ainda diz "n√£o est√° presente"

---

### Teste 3: Validar que queries boas n√£o pioraram

**Refazer Q2, Q5, Q6:**
- Devem continuar com respostas perfeitas
- Se pioraram, considerar voltar `top_n=5` e tentar Solu√ß√£o 2

---

## üí° Recomenda√ß√£o Imediata

**Pr√≥ximos passos:**

1. ‚úÖ **Deploy top_n=8** (j√° commitado)
2. ‚è≥ **Aguardar deploy Railway** (1-2 min)
3. üß™ **Testar Q1, Q3, Q4** no /chat
4. üìä **Usar /debug** para entender o que mudou
5. üìù **Reportar resultados**

**Se top_n=8 resolver 1-2 das falhas:** ‚úÖ Sucesso! √ìtimo custo-benef√≠cio.

**Se ainda falhar as 3:** Implementar Solu√ß√£o 2 (infer√™ncia moderada).

**Se Solu√ß√£o 2 n√£o resolver:** Considerar Solu√ß√£o 3 (query expansion).

---

## üéØ Meta de Sucesso

**Ideal:** 5/6 ou 6/6 perguntas corretas (83-100%)

**Aceit√°vel:** 4/6 perguntas corretas (67%) - Perguntas muito abstratas podem ser edge cases

**Inaceit√°vel:** <4/6 (67%) - Significa que retrieval est√° fundamentalmente quebrado

---

**Status atual:** 3/6 (50%) ‚Üí Precisa de melhoria

**Ap√≥s top_n=8:** Expectativa 4-5/6 (67-83%)

**Ap√≥s prompt engineering:** Expectativa 5-6/6 (83-100%)
